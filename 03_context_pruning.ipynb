{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98af1cc0",
   "metadata": {},
   "source": [
    "# Context Compression with Amazon Bedrock\n",
    "\n",
    "**Context Compression** = Reducing context size while preserving relevant information for LLM generation.\n",
    "\n",
    "Based on:\n",
    "- [How to Fix Your Context](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html) by Drew Breunig\n",
    "- [LangChain Context Engineering](https://github.com/langchain-ai/context_engineering) - Compress Context notebook\n",
    "- [Provence: Efficient and Robust Context Pruning](https://arxiv.org/abs/2501.16214) (ICLR 2025)\n",
    "\n",
    "## Context Compression Techniques\n",
    "\n",
    "| Technique | Description | Best For |\n",
    "|-----------|-------------|----------|\n",
    "| **Pruning** | Remove irrelevant sentences | Document QA, RAG |\n",
    "| **Summarization** | Condense into shorter form | Long conversations, histories |\n",
    "| **Extraction** | Pull out key facts/entities | Structured data extraction |\n",
    "| **Reranking** | Reorder by relevance | Multi-document retrieval |\n",
    "\n",
    "## Why Compress Context?\n",
    "\n",
    "Long contexts fail in several ways:\n",
    "- **Context Confusion**: Superfluous info leads to low-quality responses\n",
    "- **Context Distraction**: Model over-focuses on context, neglects training\n",
    "- **Context Poisoning**: Errors propagate through repeated references\n",
    "- **Context Clash**: New info conflicts with existing prompt content\n",
    "\n",
    "Compression addresses these by keeping only what's relevant and reducing token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 transformers torch nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d324556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import nltk\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "print('âœ“ Bedrock client ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455a780",
   "metadata": {},
   "source": [
    "## 1. Sample Document (Wikipedia-style)\n",
    "\n",
    "A long document with mixed relevant/irrelevant content for our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cf807",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DOCUMENT = \"\"\"\n",
    "Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. AWS was launched in 2006 and has since become the world's most comprehensive and broadly adopted cloud platform.\n",
    "\n",
    "The company's headquarters are located in Seattle, Washington. AWS employs over 100,000 people worldwide. The CEO of AWS is Matt Garman, who took over the role in 2024.\n",
    "\n",
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API. Amazon Bedrock makes it easy to build and scale generative AI applications with security, privacy, and responsible AI.\n",
    "\n",
    "The AWS re:Invent conference is held annually in Las Vegas, Nevada. The conference typically attracts over 50,000 attendees. The first re:Invent was held in 2012.\n",
    "\n",
    "With Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.\n",
    "\n",
    "AWS has data centers in 33 geographic regions around the world. Each region consists of multiple Availability Zones. The company continues to expand its global infrastructure.\n",
    "\n",
    "Bedrock supports several model families including Claude (Anthropic), Titan (Amazon), Llama (Meta), Command (Cohere), and Jurassic (AI21 Labs). The service provides serverless access to these models, meaning you don't need to manage any infrastructure.\n",
    "\n",
    "Amazon's founder Jeff Bezos started the company in 1994 as an online bookstore. The company went public in 1997. Today, Amazon is one of the world's most valuable companies.\n",
    "\n",
    "Key features of Amazon Bedrock include: model customization through fine-tuning, knowledge bases for RAG, agents for task automation, guardrails for responsible AI, and model evaluation capabilities. The service integrates seamlessly with other AWS services.\n",
    "\n",
    "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome. It covers most of the Amazon basin of South America. This is unrelated to Amazon the company.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(SAMPLE_DOCUMENT)} characters\")\n",
    "print(f\"Sentences: {len(nltk.sent_tokenize(SAMPLE_DOCUMENT))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745b0c2",
   "metadata": {},
   "source": [
    "## 2. Method 1: LLM-Based Pruning\n",
    "\n",
    "Use an LLM to identify and extract only relevant sentences.\n",
    "This is the approach described in the Provence paper's silver label generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2171cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def llm_prune_context(question: str, document: str, model_id: str = 'anthropic.claude-3-5-sonnet-20241022-v2:0') -> Dict:\n",
    "    \"\"\"Use LLM to prune irrelevant sentences from context.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    numbered = '\\n'.join([f'[{i}] {s}' for i, s in enumerate(sentences)])\n",
    "    \n",
    "    prompt = f\"\"\"Given the question and numbered sentences, return ONLY the sentence numbers that are relevant to answering the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sentences:\n",
    "{numbered}\n",
    "\n",
    "Return JSON array of relevant sentence numbers only. Example: [0, 2, 5]\n",
    "Relevant sentences:\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': prompt}]}],\n",
    "        inferenceConfig={'temperature': 0, 'maxTokens': 500}\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    content = response['output']['message']['content'][0]['text']\n",
    "    if '```' in content:\n",
    "        content = content.split('```')[1].replace('json', '').strip()\n",
    "    \n",
    "    try:\n",
    "        indices = json.loads(content)\n",
    "        pruned = ' '.join([sentences[i] for i in indices if i < len(sentences)])\n",
    "    except:\n",
    "        pruned = document\n",
    "        indices = list(range(len(sentences)))\n",
    "    \n",
    "    return {\n",
    "        'original_length': len(document),\n",
    "        'pruned_length': len(pruned),\n",
    "        'reduction': f\"{(1 - len(pruned)/len(document))*100:.1f}%\",\n",
    "        'kept_sentences': len(indices),\n",
    "        'total_sentences': len(sentences),\n",
    "        'pruned_text': pruned,\n",
    "        'latency': f\"{latency:.2f}s\",\n",
    "        'indices': indices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92321b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Amazon Bedrock and what models does it support?\"\n",
    "\n",
    "result = llm_prune_context(question, SAMPLE_DOCUMENT)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nOriginal: {result['original_length']} chars, {result['total_sentences']} sentences\")\n",
    "print(f\"Pruned: {result['pruned_length']} chars, {result['kept_sentences']} sentences\")\n",
    "print(f\"Reduction: {result['reduction']}\")\n",
    "print(f\"Latency: {result['latency']}\")\n",
    "print(f\"\\n--- Pruned Context ---\\n{result['pruned_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provence_intro",
   "metadata": {},
   "source": [
    "## 3. Method 2: Provence Model (Specialized Pruner)\n",
    "\n",
    "Provence is a DeBERTa-based model trained specifically for context pruning.\n",
    "It's fast (1.75GB), accurate, and works out-of-the-box across domains.\n",
    "\n",
    "Key insight from the paper: formulate pruning as **sequence labeling** - \n",
    "predict which sentences are relevant (binary mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22372505",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load Provence model\n",
    "from transformers import AutoModel\n",
    "\n",
    "print(\"Loading Provence model (this may take a moment)...\")\n",
    "provence = AutoModel.from_pretrained(\n",
    "    \"naver/provence-reranker-debertav3-v1\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ“ Provence model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28816e1a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def provence_prune_context(question: str, document: str) -> Dict:\n",
    "    \"\"\"Use Provence model for efficient context pruning.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Provence returns pruned text directly\n",
    "    pruned_text = provence.process(question, document)\n",
    "    \n",
    "    latency = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'original_length': len(document),\n",
    "        'pruned_length': len(pruned_text),\n",
    "        'reduction': f\"{(1 - len(pruned_text)/len(document))*100:.1f}%\",\n",
    "        'pruned_text': pruned_text,\n",
    "        'latency': f\"{latency:.2f}s\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e274f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Provence on the same question\n",
    "question = \"What is Amazon Bedrock and what models does it support?\"\n",
    "\n",
    "provence_result = provence_prune_context(question, SAMPLE_DOCUMENT)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nOriginal: {provence_result['original_length']} chars\")\n",
    "print(f\"Pruned: {provence_result['pruned_length']} chars\")\n",
    "print(f\"Reduction: {provence_result['reduction']}\")\n",
    "print(f\"Latency: {provence_result['latency']}\")\n",
    "print(f\"\\n--- Provence Pruned Context ---\\n{provence_result['pruned_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_intro",
   "metadata": {},
   "source": [
    "## 4. Comparison: Pruned vs Full Context QA\n",
    "\n",
    "Let's compare answer quality and latency when using:\n",
    "1. Full context (no pruning)\n",
    "2. LLM-pruned context\n",
    "3. Provence-pruned context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92abcbb1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def answer_question(question: str, context: str, model_id: str = 'anthropic.claude-3-5-sonnet-20241022-v2:0') -> Dict:\n",
    "    \"\"\"Answer a question using the provided context.\"\"\"\n",
    "    prompt = f\"\"\"Answer the question based ONLY on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': prompt}]}],\n",
    "        inferenceConfig={'temperature': 0, 'maxTokens': 1000}\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'latency': latency,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'context_length': len(context)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Amazon Bedrock and what models does it support?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Full Context vs Pruned Context\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Full context\n",
    "print(\"\\nðŸ“„ Method 1: FULL CONTEXT\")\n",
    "full_result = answer_question(question, SAMPLE_DOCUMENT)\n",
    "print(f\"Context: {full_result['context_length']} chars, {full_result['input_tokens']} tokens\")\n",
    "print(f\"Latency: {full_result['latency']:.2f}s\")\n",
    "print(f\"Answer: {full_result['answer'][:300]}...\")\n",
    "\n",
    "# 2. LLM-pruned context\n",
    "print(\"\\nâœ‚ï¸ Method 2: LLM-PRUNED CONTEXT\")\n",
    "llm_pruned = llm_prune_context(question, SAMPLE_DOCUMENT)\n",
    "llm_result = answer_question(question, llm_pruned['pruned_text'])\n",
    "print(f\"Context: {llm_result['context_length']} chars ({llm_pruned['reduction']} reduction)\")\n",
    "print(f\"Prune latency: {llm_pruned['latency']}, Answer latency: {llm_result['latency']:.2f}s\")\n",
    "print(f\"Answer: {llm_result['answer'][:300]}...\")\n",
    "\n",
    "# 3. Provence-pruned context\n",
    "print(\"\\nðŸ”¬ Method 3: PROVENCE-PRUNED CONTEXT\")\n",
    "prov_pruned = provence_prune_context(question, SAMPLE_DOCUMENT)\n",
    "prov_result = answer_question(question, prov_pruned['pruned_text'])\n",
    "print(f\"Context: {prov_result['context_length']} chars ({prov_pruned['reduction']} reduction)\")\n",
    "print(f\"Prune latency: {prov_pruned['latency']}, Answer latency: {prov_result['latency']:.2f}s\")\n",
    "print(f\"Answer: {prov_result['answer'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d74b9",
   "metadata": {},
   "source": [
    "## 5. RAG Pipeline with Context Pruning\n",
    "\n",
    "A complete RAG pipeline that incorporates context pruning:\n",
    "1. Retrieve documents\n",
    "2. Prune irrelevant content\n",
    "3. Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9828b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RAGWithPruning:\n",
    "    \"\"\"RAG pipeline with optional context pruning.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[str], use_provence: bool = True):\n",
    "        self.documents = documents\n",
    "        self.use_provence = use_provence\n",
    "        if use_provence:\n",
    "            self.pruner = provence\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"Simple keyword-based retrieval (replace with vector search in production).\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        scored = []\n",
    "        for doc in self.documents:\n",
    "            doc_words = set(doc.lower().split())\n",
    "            score = len(query_words & doc_words)\n",
    "            scored.append((score, doc))\n",
    "        \n",
    "        scored.sort(reverse=True)\n",
    "        return [doc for _, doc in scored[:top_k]]\n",
    "    \n",
    "    def prune(self, query: str, documents: List[str]) -> str:\n",
    "        \"\"\"Prune and combine retrieved documents.\"\"\"\n",
    "        pruned_docs = []\n",
    "        for doc in documents:\n",
    "            if self.use_provence:\n",
    "                pruned = self.pruner.process(query, doc)\n",
    "            else:\n",
    "                pruned = doc\n",
    "            if pruned.strip():\n",
    "                pruned_docs.append(pruned)\n",
    "        \n",
    "        return '\\n\\n'.join(pruned_docs)\n",
    "    \n",
    "    def generate(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate answer using Bedrock.\"\"\"\n",
    "        prompt = f\"\"\"Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = bedrock.converse(\n",
    "            modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
    "            messages=[{'role': 'user', 'content': [{'text': prompt}]}],\n",
    "            inferenceConfig={'temperature': 0, 'maxTokens': 1000}\n",
    "        )\n",
    "        \n",
    "        return response['output']['message']['content'][0]['text']\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Full RAG pipeline: retrieve -> prune -> generate.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        retrieved = self.retrieve(question)\n",
    "        retrieve_time = time.time() - start\n",
    "        \n",
    "        prune_start = time.time()\n",
    "        context = self.prune(question, retrieved)\n",
    "        prune_time = time.time() - prune_start\n",
    "        \n",
    "        gen_start = time.time()\n",
    "        answer = self.generate(question, context)\n",
    "        gen_time = time.time() - gen_start\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'context_length': len(context),\n",
    "            'retrieved_docs': len(retrieved),\n",
    "            'timings': {\n",
    "                'retrieve': f\"{retrieve_time:.2f}s\",\n",
    "                'prune': f\"{prune_time:.2f}s\",\n",
    "                'generate': f\"{gen_time:.2f}s\",\n",
    "                'total': f\"{time.time() - start:.2f}s\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample document corpus\n",
    "DOCUMENT_CORPUS = [\n",
    "    SAMPLE_DOCUMENT,\n",
    "    \"\"\"\n",
    "    Amazon S3 (Simple Storage Service) is an object storage service offering industry-leading scalability, data availability, security, and performance. S3 is designed for 99.999999999% (11 9's) of durability.\n",
    "    \n",
    "    S3 storage classes include S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive.\n",
    "    \n",
    "    The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, and other materials. It was built along the historical northern borders of China.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources. Lambda supports multiple programming languages including Python, Node.js, Java, Go, Ruby, and .NET.\n",
    "    \n",
    "    Lambda functions can be triggered by various AWS services including API Gateway, S3, DynamoDB, SNS, and CloudWatch Events. The maximum execution time for a Lambda function is 15 minutes.\n",
    "    \n",
    "    Pizza is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Test the RAG pipeline\n",
    "rag = RAGWithPruning(DOCUMENT_CORPUS, use_provence=True)\n",
    "\n",
    "question = \"What foundation models are available in Amazon Bedrock?\"\n",
    "result = rag.query(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nRetrieved: {result['retrieved_docs']} documents\")\n",
    "print(f\"Context length: {result['context_length']} chars\")\n",
    "print(f\"Timings: {result['timings']}\")\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4879f4e",
   "metadata": {},
   "source": [
    "## 6. Bedrock-Native Pruning (No External Model)\n",
    "\n",
    "If you can't use Provence, here's a lightweight Bedrock-only approach\n",
    "using a smaller/faster model for pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79712000",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BedrockPruner:\n",
    "    \"\"\"Context pruner using only Amazon Bedrock models.\"\"\"\n",
    "    \n",
    "    def __init__(self, pruner_model: str = 'anthropic.claude-3-haiku-20240307-v1:0'):\n",
    "        \"\"\"Use a fast, cheap model for pruning.\"\"\"\n",
    "        self.pruner_model = pruner_model\n",
    "    \n",
    "    def prune(self, question: str, document: str, threshold: float = 0.5) -> Dict:\n",
    "        \"\"\"Prune document using Bedrock model.\"\"\"\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        \n",
    "        if len(sentences) <= 3:\n",
    "            return {'pruned_text': document, 'kept': len(sentences), 'total': len(sentences)}\n",
    "        \n",
    "        prompt = f\"\"\"Rate each sentence's relevance to the question on a scale of 0-10.\n",
    "Return ONLY a JSON object with sentence indices as keys and scores as values.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sentences:\n",
    "{chr(10).join([f'{i}: {s}' for i, s in enumerate(sentences)])}\n",
    "\n",
    "JSON scores (example format: {{\"0\": 8, \"1\": 2, \"2\": 9}}):\"\"\"\n",
    "        \n",
    "        response = bedrock.converse(\n",
    "            modelId=self.pruner_model,\n",
    "            messages=[{'role': 'user', 'content': [{'text': prompt}]}],\n",
    "            inferenceConfig={'temperature': 0, 'maxTokens': 500}\n",
    "        )\n",
    "        \n",
    "        content = response['output']['message']['content'][0]['text']\n",
    "        if '```' in content:\n",
    "            content = content.split('```')[1].replace('json', '').strip()\n",
    "        \n",
    "        try:\n",
    "            scores = json.loads(content)\n",
    "            max_score = max(float(v) for v in scores.values()) if scores else 10\n",
    "            threshold_score = threshold * max_score\n",
    "            \n",
    "            kept_indices = [int(k) for k, v in scores.items() if float(v) >= threshold_score]\n",
    "            kept_indices.sort()\n",
    "            \n",
    "            pruned = ' '.join([sentences[i] for i in kept_indices if i < len(sentences)])\n",
    "        except:\n",
    "            pruned = document\n",
    "            kept_indices = list(range(len(sentences)))\n",
    "        \n",
    "        return {\n",
    "            'pruned_text': pruned,\n",
    "            'kept': len(kept_indices),\n",
    "            'total': len(sentences),\n",
    "            'reduction': f\"{(1 - len(pruned)/len(document))*100:.1f}%\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ef76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Bedrock-native pruning\n",
    "bedrock_pruner = BedrockPruner()\n",
    "\n",
    "question = \"What is Amazon Bedrock?\"\n",
    "result = bedrock_pruner.prune(question, SAMPLE_DOCUMENT)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Kept {result['kept']}/{result['total']} sentences ({result['reduction']} reduction)\")\n",
    "print(f\"\\n--- Pruned Context ---\\n{result['pruned_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a7be7",
   "metadata": {},
   "source": [
    "## 7. Measuring Pruning Quality\n",
    "\n",
    "How do we know if pruning is working well? Key metrics:\n",
    "1. **Compression ratio**: How much did we reduce?\n",
    "2. **Answer quality**: Is the answer still correct?\n",
    "3. **Latency savings**: How much faster is generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c5989",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_pruning(question: str, document: str, ground_truth: str) -> Dict:\n",
    "    \"\"\"Evaluate pruning quality across multiple methods.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Baseline: Full context\n",
    "    print(\"Testing full context...\")\n",
    "    full_start = time.time()\n",
    "    full_answer = answer_question(question, document)\n",
    "    results['full_context'] = {\n",
    "        'context_chars': len(document),\n",
    "        'latency': time.time() - full_start,\n",
    "        'answer': full_answer['answer'],\n",
    "        'input_tokens': full_answer['input_tokens']\n",
    "    }\n",
    "    \n",
    "    # LLM pruning\n",
    "    print(\"Testing LLM pruning...\")\n",
    "    llm_start = time.time()\n",
    "    llm_pruned = llm_prune_context(question, document)\n",
    "    llm_answer = answer_question(question, llm_pruned['pruned_text'])\n",
    "    results['llm_pruned'] = {\n",
    "        'context_chars': len(llm_pruned['pruned_text']),\n",
    "        'latency': time.time() - llm_start,\n",
    "        'answer': llm_answer['answer'],\n",
    "        'input_tokens': llm_answer['input_tokens'],\n",
    "        'reduction': llm_pruned['reduction']\n",
    "    }\n",
    "    \n",
    "    # Provence pruning\n",
    "    print(\"Testing Provence pruning...\")\n",
    "    prov_start = time.time()\n",
    "    prov_pruned = provence_prune_context(question, document)\n",
    "    prov_answer = answer_question(question, prov_pruned['pruned_text'])\n",
    "    results['provence_pruned'] = {\n",
    "        'context_chars': len(prov_pruned['pruned_text']),\n",
    "        'latency': time.time() - prov_start,\n",
    "        'answer': prov_answer['answer'],\n",
    "        'input_tokens': prov_answer['input_tokens'],\n",
    "        'reduction': prov_pruned['reduction']\n",
    "    }\n",
    "    \n",
    "    # Simple relevance check using LLM\n",
    "    print(\"Evaluating answer quality...\")\n",
    "    for method, data in results.items():\n",
    "        eval_prompt = f\"\"\"Rate how well this answer addresses the question (1-10).\n",
    "Also check if it contains the key information: {ground_truth}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {data['answer']}\n",
    "\n",
    "Return JSON: {{\"score\": X, \"contains_key_info\": true/false}}\"\"\"\n",
    "        \n",
    "        eval_response = bedrock.converse(\n",
    "            modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "            messages=[{'role': 'user', 'content': [{'text': eval_prompt}]}],\n",
    "            inferenceConfig={'temperature': 0, 'maxTokens': 100}\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            eval_content = eval_response['output']['message']['content'][0]['text']\n",
    "            if '```' in eval_content:\n",
    "                eval_content = eval_content.split('```')[1].replace('json', '').strip()\n",
    "            eval_result = json.loads(eval_content)\n",
    "            data['quality_score'] = eval_result.get('score', 0)\n",
    "            data['contains_key_info'] = eval_result.get('contains_key_info', False)\n",
    "        except:\n",
    "            data['quality_score'] = 'N/A'\n",
    "            data['contains_key_info'] = 'N/A'\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834362d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "question = \"What models does Amazon Bedrock support?\"\n",
    "ground_truth = \"Claude, Titan, Llama, Command, Jurassic\"\n",
    "\n",
    "eval_results = evaluate_pruning(question, SAMPLE_DOCUMENT, ground_truth)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PRUNING EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for method, data in eval_results.items():\n",
    "    print(f\"\\nðŸ“Š {method.upper()}\")\n",
    "    print(f\"   Context: {data['context_chars']} chars, {data['input_tokens']} tokens\")\n",
    "    print(f\"   Latency: {data['latency']:.2f}s\")\n",
    "    if 'reduction' in data:\n",
    "        print(f\"   Reduction: {data['reduction']}\")\n",
    "    print(f\"   Quality Score: {data['quality_score']}/10\")\n",
    "    print(f\"   Contains Key Info: {data['contains_key_info']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarization_intro",
   "metadata": {},
   "source": [
    "## 8. Context Summarization (Alternative to Pruning)\n",
    "\n",
    "For very long contexts or conversation histories, **summarization** can be more effective than pruning.\n",
    "The PokÃ©mon-playing Gemini agent found that contexts beyond 100k tokens caused the model to repeat actions rather than synthesize new plans.\n",
    "\n",
    "Key insight: Summarization preserves semantic meaning while dramatically reducing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarization_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_context(document: str, question: str = None, max_length: int = 500) -> Dict:\n",
    "    \"\"\"Summarize context while preserving information relevant to the question.\"\"\"\n",
    "    \n",
    "    if question:\n",
    "        prompt = f\"\"\"Summarize the following document, focusing on information relevant to this question: {question}\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Provide a concise summary (max {max_length} characters) that preserves key facts:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Summarize the following document, preserving all key facts and information.\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Provide a concise summary (max {max_length} characters):\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    response = bedrock.converse(\n",
    "        modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "        messages=[{'role': 'user', 'content': [{'text': prompt}]}],\n",
    "        inferenceConfig={'temperature': 0, 'maxTokens': 1000}\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    summary = response['output']['message']['content'][0]['text']\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'original_length': len(document),\n",
    "        'summary_length': len(summary),\n",
    "        'reduction': f\"{(1 - len(summary)/len(document))*100:.1f}%\",\n",
    "        'latency': f\"{latency:.2f}s\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarization_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization\n",
    "question = \"What is Amazon Bedrock and what models does it support?\"\n",
    "\n",
    "summary_result = summarize_context(SAMPLE_DOCUMENT, question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nOriginal: {summary_result['original_length']} chars\")\n",
    "print(f\"Summary: {summary_result['summary_length']} chars\")\n",
    "print(f\"Reduction: {summary_result['reduction']}\")\n",
    "print(f\"Latency: {summary_result['latency']}\")\n",
    "print(f\"\\n--- Summary ---\\n{summary_result['summary']}\")\n",
    "\n",
    "# Compare answer quality with summarized context\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Answer using summarized context:\")\n",
    "summary_answer = answer_question(question, summary_result['summary'])\n",
    "print(summary_answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pruning_vs_summarization",
   "metadata": {},
   "source": [
    "### Pruning vs Summarization\n",
    "\n",
    "| Aspect | Pruning | Summarization |\n",
    "|--------|---------|---------------|\n",
    "| **Preserves** | Original sentences | Semantic meaning |\n",
    "| **Best for** | Removing irrelevant content | Compressing long histories |\n",
    "| **Speed** | Fast (Provence) | Slower (LLM call) |\n",
    "| **Risk** | May lose context | May lose details |\n",
    "| **Use case** | RAG, document QA | Long conversations, agents |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ad24b",
   "metadata": {},
   "source": [
    "## 9. When to Use Context Pruning\n",
    "\n",
    "Context pruning is most valuable when:\n",
    "\n",
    "| Scenario | Benefit |\n",
    "|----------|---------|\n",
    "| Long retrieved documents | Reduce tokens, save cost |\n",
    "| Multiple retrieved passages | Remove redundancy |\n",
    "| Noisy retrieval results | Filter irrelevant content |\n",
    "| Latency-sensitive apps | Faster generation |\n",
    "| Edge/mobile deployment | Lower compute requirements |\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "| Method | Speed | Quality | Cost |\n",
    "|--------|-------|---------|------|\n",
    "| No pruning | Fast | Baseline | High tokens |\n",
    "| LLM pruning | Slow | High | Extra LLM call |\n",
    "| Provence | Very fast | High | Model hosting |\n",
    "| Bedrock Haiku | Medium | Good | Cheap LLM call |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd9f75",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "1. **Prune before generation, not after** - Save tokens and improve focus\n",
    "2. **Use specialized pruners for high volume** - Provence is 10-100x faster than LLM pruning\n",
    "3. **Preserve sentence order** - Maintains coherence and context flow\n",
    "4. **Set appropriate thresholds** - Too aggressive = lost information, too lenient = no benefit\n",
    "5. **Combine with other techniques**:\n",
    "   - Context Quarantine (isolate agent contexts)\n",
    "   - Tool Loadout (prune tool definitions)\n",
    "   - Context Summarization (for very long histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787b12a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Context pruning removes irrelevant information** before LLM generation  \n",
    "âœ… **Provence** is a fast, accurate, domain-agnostic pruner (ICLR 2025)  \n",
    "âœ… **LLM-based pruning** works but adds latency and cost  \n",
    "âœ… **Pruning improves quality** by reducing context confusion  \n",
    "âœ… **Pruning reduces cost** by lowering token count  \n",
    "\n",
    "### References\n",
    "\n",
    "- [How to Fix Your Context](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html) - Drew Breunig\n",
    "- [Provence: Efficient and Robust Context Pruning](https://arxiv.org/abs/2501.16214) - ICLR 2025\n",
    "- [Less is More: Optimizing Function Calling](https://arxiv.org/abs/2411.15399) - Tool pruning\n",
    "- [RAG MCP](https://arxiv.org/abs/2505.03275) - Tool selection via RAG"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
