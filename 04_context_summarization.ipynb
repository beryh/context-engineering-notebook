{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Summarization with Amazon Bedrock\n",
    "\n",
    "**Context Summarization** = Boiling down an accrued context into a condensed summary to prevent context distraction.\n",
    "\n",
    "## Why Summarize Context?\n",
    "\n",
    "1. **Context Distraction**: Beyond ~100k tokens, models favor repeating past actions over novel synthesis\n",
    "2. **Cost & Latency**: Smaller contexts = faster responses and lower costs\n",
    "3. **Focus**: Preserves what matters, removes noise\n",
    "\n",
    "**Reference**: [How to Fix Your Context](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "MODEL_ID = \"anthropic.claude-3-5-sonnet-20241022-v2:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Summarization Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, focus: str = None, max_tokens: int = 500) -> str:\n",
    "    \"\"\"Summarize text with optional focus area.\"\"\"\n",
    "    focus_instruction = f\" Focus on: {focus}\" if focus else \"\"\n",
    "    \n",
    "    response = bedrock.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": \n",
    "            f\"Summarize this concisely, preserving key facts and decisions.{focus_instruction}\\n\\n{text}\"\n",
    "        }]}],\n",
    "        inferenceConfig={\"temperature\": 0.3, \"maxTokens\": max_tokens}\n",
    "    )\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conversation Summarizer\n",
    "\n",
    "Summarize conversation history when it grows too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummarizer:\n",
    "    \"\"\"Manages conversation with automatic summarization.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str, max_messages: int = 10):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_messages = max_messages\n",
    "        self.messages = []\n",
    "        self.summary = None\n",
    "        \n",
    "    def _summarize_history(self) -> str:\n",
    "        \"\"\"Create summary of conversation history.\"\"\"\n",
    "        history = \"\\n\".join([\n",
    "            f\"{m['role'].upper()}: {m['content'][0]['text'][:200]}...\" \n",
    "            for m in self.messages\n",
    "        ])\n",
    "        \n",
    "        response = bedrock.converse(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": \n",
    "                f\"Summarize this conversation, preserving: key decisions, facts, user preferences, and current task state.\\n\\n{history}\"\n",
    "            }]}],\n",
    "            inferenceConfig={\"temperature\": 0.3, \"maxTokens\": 400}\n",
    "        )\n",
    "        return response['output']['message']['content'][0]['text']\n",
    "    \n",
    "    def _maybe_compress(self):\n",
    "        \"\"\"Compress if messages exceed threshold.\"\"\"\n",
    "        if len(self.messages) >= self.max_messages:\n",
    "            self.summary = self._summarize_history()\n",
    "            # Keep only last 2 exchanges\n",
    "            self.messages = self.messages[-4:]\n",
    "            print(f\"ðŸ“¦ Compressed! Summary: {self.summary[:100]}...\")\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Send message and get response.\"\"\"\n",
    "        self._maybe_compress()\n",
    "        \n",
    "        # Build system with summary if exists\n",
    "        system = self.system_prompt\n",
    "        if self.summary:\n",
    "            system += f\"\\n\\nPrevious conversation summary:\\n{self.summary}\"\n",
    "        \n",
    "        self.messages.append({\"role\": \"user\", \"content\": [{\"text\": user_input}]})\n",
    "        \n",
    "        response = bedrock.converse(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=self.messages,\n",
    "            system=[{\"text\": system}],\n",
    "            inferenceConfig={\"temperature\": 0.7, \"maxTokens\": 1000}\n",
    "        )\n",
    "        \n",
    "        answer = response['output']['message']['content'][0]['text']\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": [{\"text\": answer}]})\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def stats(self):\n",
    "        return {\"messages\": len(self.messages), \"has_summary\": self.summary is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Conversation with auto-summarization\n",
    "chat = ConversationSummarizer(\n",
    "    \"You are a helpful coding assistant.\",\n",
    "    max_messages=6  # Low threshold for demo\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"What's the best way to handle errors in Python?\",\n",
    "    \"Can you show me a try-except example?\",\n",
    "    \"What about custom exceptions?\",\n",
    "    \"How do I log errors properly?\",\n",
    "    \"What logging levels should I use?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nðŸ‘¤ {q}\")\n",
    "    response = chat.chat(q)\n",
    "    print(f\"ðŸ¤– {response[:150]}...\")\n",
    "    print(f\"   Stats: {chat.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Summarization\n",
    "\n",
    "For very long documents: summarize chunks, then summarize summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 2000) -> List[str]:\n",
    "    \"\"\"Split text into chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunks.append(' '.join(words[i:i + chunk_size]))\n",
    "    return chunks\n",
    "\n",
    "def hierarchical_summarize(text: str, focus: str = None) -> Dict:\n",
    "    \"\"\"Summarize long text hierarchically.\"\"\"\n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    if len(chunks) == 1:\n",
    "        return {\"summary\": summarize_text(text, focus), \"chunks\": 1, \"levels\": 1}\n",
    "    \n",
    "    # Level 1: Summarize each chunk\n",
    "    chunk_summaries = [summarize_text(c, focus, max_tokens=300) for c in chunks]\n",
    "    print(f\"ðŸ“„ Summarized {len(chunks)} chunks\")\n",
    "    \n",
    "    # Level 2: Combine summaries\n",
    "    combined = \"\\n\\n\".join([f\"Section {i+1}: {s}\" for i, s in enumerate(chunk_summaries)])\n",
    "    final = summarize_text(combined, focus, max_tokens=500)\n",
    "    \n",
    "    return {\n",
    "        \"summary\": final,\n",
    "        \"chunk_summaries\": chunk_summaries,\n",
    "        \"chunks\": len(chunks),\n",
    "        \"levels\": 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with sample long text\n",
    "long_text = \"\"\"\n",
    "Amazon Web Services (AWS) provides a comprehensive cloud computing platform. \n",
    "The platform includes compute services like EC2 for virtual servers and Lambda for serverless functions.\n",
    "Storage options include S3 for object storage, EBS for block storage, and EFS for file systems.\n",
    "Database services range from RDS for relational databases to DynamoDB for NoSQL.\n",
    "Networking is handled through VPC, Route 53 for DNS, and CloudFront for CDN.\n",
    "Security features include IAM for access management, KMS for encryption, and WAF for web protection.\n",
    "Machine learning services include SageMaker for model training and Bedrock for foundation models.\n",
    "Analytics services include Athena for queries, Redshift for warehousing, and Kinesis for streaming.\n",
    "Developer tools include CodePipeline, CodeBuild, and CodeDeploy for CI/CD workflows.\n",
    "Management tools include CloudWatch for monitoring, CloudFormation for infrastructure as code.\n",
    "\"\"\" * 10  # Repeat to make it longer\n",
    "\n",
    "result = hierarchical_summarize(long_text, focus=\"key AWS services\")\n",
    "print(f\"\\nðŸ“Š Result: {result['chunks']} chunks, {result['levels']} levels\")\n",
    "print(f\"\\nðŸ“ Final Summary:\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Context Manager\n",
    "\n",
    "Maintain structured context with selective summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredContextManager:\n",
    "    \"\"\"Manages context with separate sections that can be individually summarized.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sections = {\n",
    "            \"goals\": [],      # Never summarize - always keep full\n",
    "            \"facts\": [],      # Summarize when large\n",
    "            \"history\": [],    # Aggressively summarize\n",
    "            \"scratchpad\": []  # Temporary, can be cleared\n",
    "        }\n",
    "        self.summaries = {}\n",
    "        \n",
    "    def add(self, section: str, content: str):\n",
    "        \"\"\"Add content to a section.\"\"\"\n",
    "        if section in self.sections:\n",
    "            self.sections[section].append(content)\n",
    "            \n",
    "    def _summarize_section(self, section: str, threshold: int) -> str:\n",
    "        \"\"\"Summarize a section if it exceeds threshold.\"\"\"\n",
    "        content = \"\\n\".join(self.sections[section])\n",
    "        if len(content) > threshold:\n",
    "            summary = summarize_text(content, max_tokens=300)\n",
    "            self.summaries[section] = summary\n",
    "            self.sections[section] = []  # Clear after summarizing\n",
    "            return summary\n",
    "        return content\n",
    "    \n",
    "    def compile_context(self) -> str:\n",
    "        \"\"\"Compile all sections into final context string.\"\"\"\n",
    "        # Summarize sections that need it\n",
    "        self._summarize_section(\"facts\", threshold=2000)\n",
    "        self._summarize_section(\"history\", threshold=1000)\n",
    "        \n",
    "        parts = []\n",
    "        \n",
    "        # Goals: always full\n",
    "        if self.sections[\"goals\"]:\n",
    "            parts.append(f\"## Goals\\n\" + \"\\n\".join(self.sections[\"goals\"]))\n",
    "        \n",
    "        # Facts: use summary if available\n",
    "        facts = self.summaries.get(\"facts\") or \"\\n\".join(self.sections[\"facts\"])\n",
    "        if facts:\n",
    "            parts.append(f\"## Key Facts\\n{facts}\")\n",
    "        \n",
    "        # History: use summary if available\n",
    "        history = self.summaries.get(\"history\") or \"\\n\".join(self.sections[\"history\"])\n",
    "        if history:\n",
    "            parts.append(f\"## History Summary\\n{history}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(parts)\n",
    "    \n",
    "    def clear_scratchpad(self):\n",
    "        \"\"\"Clear temporary notes.\"\"\"\n",
    "        self.sections[\"scratchpad\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Structured context\n",
    "ctx = StructuredContextManager()\n",
    "\n",
    "# Add goals (never summarized)\n",
    "ctx.add(\"goals\", \"Build a REST API for user management\")\n",
    "ctx.add(\"goals\", \"Use Python with FastAPI framework\")\n",
    "\n",
    "# Add facts (summarized when large)\n",
    "ctx.add(\"facts\", \"Database: PostgreSQL on RDS\")\n",
    "ctx.add(\"facts\", \"Auth: JWT tokens with 1-hour expiry\")\n",
    "ctx.add(\"facts\", \"Deployment: ECS Fargate\")\n",
    "\n",
    "# Add history (aggressively summarized)\n",
    "ctx.add(\"history\", \"Created project structure\")\n",
    "ctx.add(\"history\", \"Set up database models\")\n",
    "ctx.add(\"history\", \"Implemented user CRUD endpoints\")\n",
    "ctx.add(\"history\", \"Added authentication middleware\")\n",
    "\n",
    "print(\"ðŸ“‹ Compiled Context:\")\n",
    "print(ctx.compile_context())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Progressive Summarization Agent\n",
    "\n",
    "An agent that progressively summarizes as it works through a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveSummarizationAgent:\n",
    "    \"\"\"Agent that maintains a running summary of its work.\"\"\"\n",
    "    \n",
    "    def __init__(self, task: str):\n",
    "        self.task = task\n",
    "        self.running_summary = f\"Task: {task}\"\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def execute_step(self, step_description: str) -> str:\n",
    "        \"\"\"Execute a step and update running summary.\"\"\"\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Execute the step\n",
    "        response = bedrock.converse(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": \n",
    "                f\"Context:\\n{self.running_summary}\\n\\nExecute this step: {step_description}\"\n",
    "            }]}],\n",
    "            inferenceConfig={\"temperature\": 0.7, \"maxTokens\": 800}\n",
    "        )\n",
    "        result = response['output']['message']['content'][0]['text']\n",
    "        \n",
    "        # Update running summary (compress old + add new)\n",
    "        self._update_summary(step_description, result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _update_summary(self, step: str, result: str):\n",
    "        \"\"\"Update running summary with new step.\"\"\"\n",
    "        update_prompt = f\"\"\"Current summary:\n",
    "{self.running_summary}\n",
    "\n",
    "New step completed: {step}\n",
    "Result: {result[:500]}\n",
    "\n",
    "Create an updated summary that:\n",
    "1. Preserves the original task\n",
    "2. Keeps key decisions and findings\n",
    "3. Notes current progress\n",
    "4. Stays under 300 words\"\"\"\n",
    "        \n",
    "        response = bedrock.converse(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": update_prompt}]}],\n",
    "            inferenceConfig={\"temperature\": 0.3, \"maxTokens\": 400}\n",
    "        )\n",
    "        self.running_summary = response['output']['message']['content'][0]['text']\n",
    "        \n",
    "    def get_summary(self) -> str:\n",
    "        return f\"Steps completed: {self.step_count}\\n\\n{self.running_summary}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Progressive summarization\n",
    "agent = ProgressiveSummarizationAgent(\"Design a caching strategy for an e-commerce API\")\n",
    "\n",
    "steps = [\n",
    "    \"Identify what data should be cached (products, user sessions, cart)\",\n",
    "    \"Choose caching technology (Redis vs Memcached)\",\n",
    "    \"Define cache invalidation strategy\",\n",
    "    \"Plan cache warming approach\"\n",
    "]\n",
    "\n",
    "for step in steps:\n",
    "    print(f\"\\nðŸ”§ Step: {step}\")\n",
    "    result = agent.execute_step(step)\n",
    "    print(f\"âœ… Result: {result[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(agent.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Summarize proactively** - Don't wait until context is full  \n",
    "âœ… **Preserve what matters** - Goals, decisions, key facts  \n",
    "âœ… **Use structure** - Different sections need different treatment  \n",
    "âœ… **Hierarchical approach** - For very long content, summarize in levels  \n",
    "âœ… **Running summaries** - Update as you go, not all at once  \n",
    "\n",
    "**When to use:**\n",
    "- Long conversations approaching context limits\n",
    "- Multi-step agent tasks\n",
    "- Processing large documents\n",
    "- When you notice quality degradation\n",
    "\n",
    "**References:**\n",
    "- [How to Fix Your Context](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html)\n",
    "- [LangChain Context Engineering](https://github.com/langchain-ai/context_engineering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}